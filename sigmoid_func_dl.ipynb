{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfd1208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        RI     Na    Mg    Al     Si     K    Ca   Ba   Fe  Type\n",
      "1  1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.0  0.0     1\n",
      "2  1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.0  0.0     1\n",
      "3  1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.0  0.0     1\n",
      "4  1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.0  0.0     1\n",
      "5  1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.0  0.0     1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "columns = ['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe', 'Type']\n",
    "df = pd.read_csv(\n",
    "    \"glass+identification/glass.data\",\n",
    "    header=None,\n",
    "    names=columns\n",
    ")\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3409ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "y = np.where(y == 1, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b02856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use Z-score normalization (also called standardization).\n",
    "def normalize(X):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    return (X - mean) / std\n",
    "\n",
    "X = normalize(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d74ba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, train_ratio=0.7, val_ratio=0.15):\n",
    "    n = X.shape[0]\n",
    "    indices = np.random.permutation(n)\n",
    "\n",
    "    train_end = int(train_ratio * n)\n",
    "    val_end = int((train_ratio + val_ratio) * n)\n",
    "\n",
    "    X_train = X[indices[:train_end]]\n",
    "    y_train = y[indices[:train_end]]\n",
    "\n",
    "    X_val = X[indices[train_end:val_end]]\n",
    "    y_val = y[indices[train_end:val_end]]\n",
    "\n",
    "    X_test = X[indices[val_end:]]\n",
    "    y_test = y[indices[val_end:]]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5c68508",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = split_data(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ee0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#“The sigmoid function maps the linear output of logistic regression into a \n",
    "# probability between 0 and 1, \n",
    "# making it suitable for binary classification.”\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca91041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computes the predicted probability that an input belongs to class 1.\n",
    "#“The predict_prob function computes the linear combination of inputs and weights, adds bias, \n",
    "# and applies the sigmoid function to obtain class probabilities.”\n",
    "def predict_prob(X, weights, bias):\n",
    "    z = np.dot(X, weights) + bias\n",
    "    return sigmoid(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It measures how far the predicted probabilities are from the true labels.\n",
    "#“Binary cross-entropy loss measures the difference between true labels \n",
    "# and predicted probabilities and heavily penalizes confident incorrect \n",
    "# predictions, making it suitable for logistic regression.”\n",
    "def loss(y, y_hat):\n",
    "    epsilon = 1e-9\n",
    "    return -np.mean(y * np.log(y_hat + epsilon) +\n",
    "                    (1 - y) * np.log(1 - y_hat + epsilon))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ca57df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to learn the best weights and bias that minimize the loss function.\n",
    "def train(X, y, lr=0.01, epochs=1000):\n",
    "    n_features = X.shape[1]\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        y_hat = predict_prob(X, weights, bias)\n",
    "\n",
    "        dw = np.dot(X.T, (y_hat - y)) / len(y)\n",
    "        db = np.mean(y_hat - y)\n",
    "\n",
    "        weights -= lr * dw\n",
    "        bias -= lr * db\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss(y, y_hat):.4f}\")\n",
    "\n",
    "    return weights, bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bcdf06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6931\n",
      "Epoch 100, Loss: 0.6080\n",
      "Epoch 200, Loss: 0.5663\n",
      "Epoch 300, Loss: 0.5414\n",
      "Epoch 400, Loss: 0.5245\n",
      "Epoch 500, Loss: 0.5121\n",
      "Epoch 600, Loss: 0.5026\n",
      "Epoch 700, Loss: 0.4950\n",
      "Epoch 800, Loss: 0.4888\n",
      "Epoch 900, Loss: 0.4836\n"
     ]
    }
   ],
   "source": [
    "weights, bias = train(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f7da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#“After training, predicted probabilities are converted into class labels using a \n",
    "# threshold of 0.5, and model performance is evaluated using accuracy on training,\n",
    "#  validation, and test datasets.”\n",
    "def predict(X, weights, bias):\n",
    "    probs = predict_prob(X, weights, bias)\n",
    "    return np.where(probs >= 0.5, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fece1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8a17e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7315436241610739\n",
      "Validation Accuracy: 0.875\n",
      "Test Accuracy: 0.696969696969697\n"
     ]
    }
   ],
   "source": [
    "train_acc = accuracy(y_train, predict(X_train, weights, bias))\n",
    "val_acc = accuracy(y_val, predict(X_val, weights, bias))\n",
    "test_acc = accuracy(y_test, predict(X_test, weights, bias))\n",
    "\n",
    "print(\"Train Accuracy:\", train_acc)\n",
    "print(\"Validation Accuracy:\", val_acc)\n",
    "print(\"Test Accuracy:\", test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
